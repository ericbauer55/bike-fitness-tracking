{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c754fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from haversine import haversine\n",
    "CSV_DATA = Path().cwd().parent.parent / \"data/0_extracted/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e882109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ride_csv(file_path:str, time_columns:list[str]=None):\n",
    "    if time_columns is None: time_columns=['time']\n",
    "    # Read in the CSV file for the Ride\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # guarantee the timestamps are datetime objects\n",
    "    for time_col in time_columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703f79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_ride_path = CSV_DATA / \"January_no_sensor.csv\"\n",
    "apr_ride_path = CSV_DATA / \"April_one_sensor.csv\"\n",
    "may_ride_path = CSV_DATA / \"May_two_sensor.csv\"\n",
    "df_jan = read_ride_csv(jan_ride_path)\n",
    "df_apr = read_ride_csv(apr_ride_path)\n",
    "df_may = read_ride_csv(may_ride_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760194d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['ride_id', 'track_id']\n",
    "df_jan = df_jan.drop(columns=drop_columns)\n",
    "df_apr = df_apr.drop(columns=drop_columns)\n",
    "df_may = df_may.drop(columns=drop_columns)\n",
    "add_null_columns = ['atemp', 'hr'] # if these columns don't already exist add them as np.null\n",
    "\n",
    "for df in [df_jan, df_apr, df_may]:\n",
    "    for col in add_null_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58753534",
   "metadata": {},
   "source": [
    "# 1. Enrich & Upsample Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e557e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeUpsampler:\n",
    "    def process(self, df:pd.DataFrame, time_gap_threshold:int=15) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df = self.enrich_time_data(df)\n",
    "        df['elapsed_time'] = df['delta_time'].cumsum()\n",
    "        df = self.label_continuous_segments(df, time_gap_threshold)\n",
    "        df_upsampled = self.normalize_sampling_rate(df)\n",
    "        return df_upsampled\n",
    "\n",
    "    @staticmethod\n",
    "    def enrich_time_data(df:pd.DataFrame, time_column:str='time', fill_first:float=1.0):\n",
    "        df = df.copy()\n",
    "        # Temporarily get the number of seconds since Jan. 1, 1970 as the UTC timestamp\n",
    "        df['time_utc'] = df[time_column].apply(lambda x: x.timestamp())\n",
    "        \n",
    "        # Calculate the row-wise difference in time (in seconds)\n",
    "        df['delta_time'] = df['time_utc'].diff()\n",
    "        \n",
    "        # drop the temporary time column\n",
    "        df.drop(['time_utc'], axis=1, inplace=True)\n",
    "        \n",
    "        # fill in the initial value of delta_time with @fill_first\n",
    "        df['delta_time'] = df['delta_time'].fillna(fill_first)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def label_continuous_segments(df: pd.DataFrame, time_gap_threshold:int=15):\n",
    "        df = df.copy()\n",
    "        # get the time gap indices\n",
    "        # Calculate when the time discontinuities occur\n",
    "        filt_time_jump = df['delta_time'] >= time_gap_threshold\n",
    "        time_gap_indices = list(df.loc[filt_time_jump, 'time'].index)\n",
    "\n",
    "        # intialize the initial segment_id. to be incremented for each region of continuous data\n",
    "        segment_id_counter = 0\n",
    "        # initialize the starting index of the first segment\n",
    "        segment_start_index = 0\n",
    "\n",
    "        for time_gap_index in time_gap_indices:\n",
    "            # Assign the Segment ID\n",
    "            df.loc[segment_start_index:time_gap_index-1, 'segment_id'] = segment_id_counter\n",
    "            \n",
    "            # update the segment_id counter and start index\n",
    "            segment_id_counter += 1\n",
    "            segment_start_index = time_gap_index\n",
    "            \n",
    "        # Since segment_id == -1 by default, this represents the final segment of activity once parsed\n",
    "        df['segment_id'] = df['segment_id'].replace({-1:segment_id_counter})\n",
    "\n",
    "        # Since the delta_time column is no longer needed to detect discontinuities,\n",
    "        # Drop delta_time so we can rebuild it at a segment_id level\n",
    "        df.drop(['delta_time'], axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def upsample_and_interpolate(df: pd.DataFrame, time_column:str='time', method:str='linear', limit_direction:str='forward'):\n",
    "        # set the timestamp as the index for the dataframe\n",
    "        df = df.set_index(time_column).copy()\n",
    "        df = df.resample('s').interpolate(method=method, limit_direction=limit_direction).reset_index()\n",
    "        return df\n",
    "    \n",
    "    def normalize_sampling_rate(self, df: pd.DataFrame, partition_column:str='segment_id') -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        functions_to_apply = [self.upsample_and_interpolate, self.enrich_time_data]\n",
    "        for func in functions_to_apply:\n",
    "            df = pd.concat(list(map(func, [df_group for _,df_group in df.groupby(partition_column)])), ignore_index=True).sort_index()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9378061",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampler = TimeUpsampler()\n",
    "df_jan = upsampler.process(df_jan)\n",
    "df_apr = upsampler.process(df_apr)\n",
    "df_may = upsampler.process(df_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509a88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "class BasicEnricher:\n",
    "    def process(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        process_steps = [self.compute_distance, self.compute_heading, self.compute_speed, self.flag_cruising_rows, \n",
    "                         self.convert_elevation, self.compute_grade, self.compute_cumulative_elevation_changes]\n",
    "        \n",
    "        df_enriched = reduce(lambda df,func:func(df), process_steps, df)\n",
    "        return df_enriched\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Distance and Heading from Lat/Long\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def compute_distance(df, latitude='latitude', longitude='longitude', fill_first=np.nan):\n",
    "        df = df.copy()\n",
    "        # Copy the previous values of Lat/Long to the current row for vectorized computation\n",
    "        df['lat_old'] = df[latitude].shift()\n",
    "        df['long_old'] = df[longitude].shift()\n",
    "        \n",
    "        # Grab the relevant columns for distance calculation\n",
    "        df_gps = df[['lat_old', 'long_old', latitude, longitude]]\n",
    "        \n",
    "        # Define an anonymous function to execute over each row to calculate the distance between rows\n",
    "        haversine_distance = lambda x: haversine((x.iloc[0], x.iloc[1]), (x.iloc[2], x.iloc[3]), unit='mi')\n",
    "        \n",
    "        # Create the distance column, making sure to apply the function row-by-row\n",
    "        df['delta_dist'] = df_gps.apply(haversine_distance, axis=1)\n",
    "        df['delta_dist'] = df['delta_dist'].fillna(fill_first)\n",
    "        \n",
    "        # Remove the old latitude and longitude columns\n",
    "        df.drop(['lat_old','long_old'], axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_heading(df, latitude='latitude', longitude='longitude'):\n",
    "        df = df.copy()\n",
    "        # Copy the previous values of Lat/Long to the current row for vectorized computation\n",
    "        df['lat_old'] = df[latitude].shift()\n",
    "        df['long_old'] = df[longitude].shift()\n",
    "        \n",
    "        # Grab the relevant columns for distance calculation\n",
    "        df_gps = df[['lat_old', 'long_old', latitude, longitude]]\n",
    "        \n",
    "        # Define an anonymous function to execute over each row to calculate the angle with North as 0 degrees\n",
    "        # NOTE: we use \"delta_lat / delta_long\" to ensure that North = 0 degrees\n",
    "        rad2deg = 180.0 / np.pi\n",
    "        heading = lambda x: rad2deg * np.arctan2((x.iloc[2]-x.iloc[0]), (x.iloc[3]-x.iloc[1])) # atan(delta_lat / delta_long)\n",
    "        \n",
    "        # Create the distance column, making sure to apply the function row-by-row\n",
    "        df['heading'] = df_gps.apply(heading, axis=1)\n",
    "        df['heading'] = df['heading'].apply(lambda x: x + 360.0*(1-np.sign(x))/2) # correct for negative angles\n",
    "        \n",
    "        # Remove the old latitude and longitude columns\n",
    "        df.drop(['lat_old','long_old'], axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Speed Enrichments\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def compute_speed(df):\n",
    "        df = df.copy()\n",
    "        miles_per_second_2_MPH = 3600.0 / 1.0 # conversion factor\n",
    "        df['speed'] = miles_per_second_2_MPH * df['delta_dist'] / df['delta_time']\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def flag_cruising_rows(df, start_threshold_mph:float=8.0, stop_threshold_mph:float=5.0):\n",
    "        \"\"\"\n",
    "        Scmitt Trigger to implement a hysteresis state machine for determining a state\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['is_cruising'] = False\n",
    "\n",
    "        for k in range(1, df.shape[0]):\n",
    "            previous_state = df.loc[k-1,'is_cruising']\n",
    "            current_speed = df.loc[k,'speed']\n",
    "            if (previous_state==False) & (current_speed >= start_threshold_mph):\n",
    "                df.loc[k,'is_cruising'] = True # rising threshold surpassed\n",
    "            elif (previous_state==True) & (current_speed < stop_threshold_mph):\n",
    "                df.loc[k,'is_cruising'] = False # rising threshold surpassed\n",
    "            else:\n",
    "                # if there is no change, propogate the previous state\n",
    "                df.loc[k,'is_cruising'] = df.loc[k-1,'is_cruising']\n",
    "        return df\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Elevation Enrichments\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def convert_elevation(df):\n",
    "        df = df.copy()\n",
    "        meters_to_feet = 3.281\n",
    "        df['elevation'] = df['elevation'] * meters_to_feet\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_grade(df, fill_first=0.0):\n",
    "        df = df.copy()\n",
    "        # create an elevation difference\n",
    "        feet_to_miles = 1.0 / 5280.0\n",
    "        df['delta_ele'] = df['elevation'].diff() * feet_to_miles\n",
    "        df['delta_ele'] = df['delta_ele'].fillna(fill_first)\n",
    "        \n",
    "        # create the grade column as a percent\n",
    "        df['grade'] = 100.0 * (df['delta_ele'] / df['delta_dist'])\n",
    "        \n",
    "        # drop the elevation difference\n",
    "        df.drop(['delta_ele'], axis=1, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cumulative_elevation_changes(df, fill_first=0.0):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # create an elevation difference\n",
    "        df['delta_ele'] = df['elevation'].diff()\n",
    "        df['delta_ele'] = df['delta_ele'].fillna(fill_first)\n",
    "        \n",
    "        # create delta ascent and delta descent columns\n",
    "        df['delta_ascent'] = df.loc[df['delta_ele']>=0, 'delta_ele']\n",
    "        df['delta_descent'] = df.loc[df['delta_ele']<=0, 'delta_ele']\n",
    "        \n",
    "        # create the cumulative versions\n",
    "        df['elapsed_ascent'] = df['delta_ascent'].cumsum()\n",
    "        df['elapsed_ascent'] = df['elapsed_ascent'].interpolate() # fill in any blanks\n",
    "        df['elapsed_descent'] = df['delta_descent'].cumsum()\n",
    "        df['elapsed_descent'] = np.abs(df['elapsed_descent'].interpolate()) # fill in any blanks\n",
    "        \n",
    "        # create the total elevation change column\n",
    "        df['elapsed_elevation'] = df['elapsed_ascent'] + df['elapsed_descent']\n",
    "            \n",
    "        # drop the elevation differences\n",
    "        df.drop(['delta_ele','delta_ascent','delta_descent'], axis=1, inplace=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f304d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enricher = BasicEnricher()\n",
    "df_jan = enricher.process(df_jan)\n",
    "df_apr = enricher.process(df_apr)\n",
    "df_may = enricher.process(df_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb29414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>elevation</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hr</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>delta_time</th>\n",
       "      <th>delta_dist</th>\n",
       "      <th>heading</th>\n",
       "      <th>speed</th>\n",
       "      <th>is_cruising</th>\n",
       "      <th>grade</th>\n",
       "      <th>elapsed_ascent</th>\n",
       "      <th>elapsed_descent</th>\n",
       "      <th>elapsed_elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-12 17:21:13+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.6194</td>\n",
       "      <td>39.139397</td>\n",
       "      <td>-84.341631</td>\n",
       "      <td>29.0</td>\n",
       "      <td>93.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-12 17:21:14+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.6194</td>\n",
       "      <td>39.139397</td>\n",
       "      <td>-84.341631</td>\n",
       "      <td>29.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-12 17:21:15+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.6194</td>\n",
       "      <td>39.139397</td>\n",
       "      <td>-84.341631</td>\n",
       "      <td>29.0</td>\n",
       "      <td>94.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-12 17:21:16+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.6194</td>\n",
       "      <td>39.139397</td>\n",
       "      <td>-84.341631</td>\n",
       "      <td>29.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-12 17:21:17+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.6194</td>\n",
       "      <td>39.139397</td>\n",
       "      <td>-84.341631</td>\n",
       "      <td>29.0</td>\n",
       "      <td>89.25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  segment_id  elevation   latitude  longitude  \\\n",
       "0 2025-04-12 17:21:13+00:00         0.0   483.6194  39.139397 -84.341631   \n",
       "1 2025-04-12 17:21:14+00:00         0.0   483.6194  39.139397 -84.341631   \n",
       "2 2025-04-12 17:21:15+00:00         0.0   483.6194  39.139397 -84.341631   \n",
       "3 2025-04-12 17:21:16+00:00         0.0   483.6194  39.139397 -84.341631   \n",
       "4 2025-04-12 17:21:17+00:00         0.0   483.6194  39.139397 -84.341631   \n",
       "\n",
       "   atemp     hr  elapsed_time  delta_time  delta_dist  heading  speed  \\\n",
       "0   29.0  93.00           1.0         1.0         NaN      NaN    NaN   \n",
       "1   29.0  91.00           2.0         1.0         0.0    180.0    0.0   \n",
       "2   29.0  94.00           3.0         1.0         0.0    180.0    0.0   \n",
       "3   29.0  90.00           4.0         1.0         0.0    180.0    0.0   \n",
       "4   29.0  89.25           5.0         1.0         0.0    180.0    0.0   \n",
       "\n",
       "   is_cruising  grade  elapsed_ascent  elapsed_descent  elapsed_elevation  \n",
       "0        False    NaN             0.0              0.0                0.0  \n",
       "1        False    NaN             0.0              0.0                0.0  \n",
       "2        False    NaN             0.0              0.0                0.0  \n",
       "3        False    NaN             0.0              0.0                0.0  \n",
       "4        False    NaN             0.0              0.0                0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_apr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcd942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03c728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdca889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d6b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e80e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
