{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea881d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from haversine import haversine\n",
    "CSV_DATA = Path().cwd().parent.parent / \"data/0_extracted/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ride_csv(file_path:str, time_columns:list[str]=None):\n",
    "    if time_columns is None: time_columns=['time']\n",
    "    # Read in the CSV file for the Ride\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # guarantee the timestamps are datetime objects\n",
    "    for time_col in time_columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    drop_columns = ['ride_id', 'track_id']\n",
    "    df = df.drop(columns=drop_columns)\n",
    "\n",
    "    add_null_columns = ['atemp', 'hr'] # if these columns don't already exist add them as np.null\n",
    "\n",
    "    for col in add_null_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_ride_path = CSV_DATA / \"January_no_sensor.csv\"\n",
    "apr_ride_path = CSV_DATA / \"April_one_sensor.csv\"\n",
    "may_ride_path = CSV_DATA / \"May_two_sensor.csv\"\n",
    "df_jan = read_ride_csv(jan_ride_path)\n",
    "df_apr = read_ride_csv(apr_ride_path)\n",
    "df_may = read_ride_csv(may_ride_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_column(df:pd.DataFrame, column:str, color_by:str='segment_id',figsize=(15,3),\n",
    "                       ylims:list[float]=None, units:str='none') -> None:\n",
    "    _ = plt.figure(figsize=figsize)\n",
    "    _ = sn.lineplot(df, x='elapsed_time', y=column, hue=color_by, palette='hls', legend=False)\n",
    "    _ = plt.grid()\n",
    "    _ = plt.title(f'{column.upper()} Segments', fontsize=18)\n",
    "    _ = plt.xlabel('Elapsed Time (seconds)', fontsize=14)\n",
    "    _ = plt.ylabel(f'{column.upper()} ({units})', fontsize=14)\n",
    "    if ylims is not None:\n",
    "        _ = plt.ylim(ylims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeUpsampler:\n",
    "    def process(self, df:pd.DataFrame, time_gap_threshold:int=15, upsample:bool=False) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df = self.enrich_time_data(df)\n",
    "        df['elapsed_time'] = df['delta_time'].cumsum()\n",
    "        df = self.label_continuous_segments(df, time_gap_threshold)\n",
    "        if upsample:\n",
    "            return self.upsample(df)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    def upsample(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        df_upsampled = self.normalize_sampling_rate(df)\n",
    "        return df_upsampled\n",
    "\n",
    "    @staticmethod\n",
    "    def enrich_time_data(df:pd.DataFrame, time_column:str='time', fill_first:float=1.0):\n",
    "        df = df.copy()\n",
    "        # Temporarily get the number of seconds since Jan. 1, 1970 as the UTC timestamp\n",
    "        df['time_utc'] = df[time_column].apply(lambda x: x.timestamp())\n",
    "        \n",
    "        # Calculate the row-wise difference in time (in seconds)\n",
    "        df['delta_time'] = df['time_utc'].diff()\n",
    "        \n",
    "        # drop the temporary time column\n",
    "        df.drop(['time_utc'], axis=1, inplace=True)\n",
    "        \n",
    "        # fill in the initial value of delta_time with @fill_first\n",
    "        df['delta_time'] = df['delta_time'].fillna(fill_first)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def label_continuous_segments(df: pd.DataFrame, time_gap_threshold:int=15):\n",
    "        df = df.copy()\n",
    "        # get the time gap indices\n",
    "        # Calculate when the time discontinuities occur\n",
    "        filt_time_jump = df['delta_time'] >= time_gap_threshold\n",
    "        time_gap_indices = list(df.loc[filt_time_jump, 'time'].index)\n",
    "\n",
    "        # intialize the initial segment_id. to be incremented for each region of continuous data\n",
    "        segment_id_counter = 0\n",
    "        # initialize the starting index of the first segment\n",
    "        segment_start_index = 0\n",
    "\n",
    "        for time_gap_index in time_gap_indices:\n",
    "            # Assign the Segment ID\n",
    "            df.loc[segment_start_index:time_gap_index-1, 'segment_id'] = segment_id_counter\n",
    "            \n",
    "            # update the segment_id counter and start index\n",
    "            segment_id_counter += 1\n",
    "            segment_start_index = time_gap_index\n",
    "            \n",
    "        # Since segment_id == -1 by default, this represents the final segment of activity once parsed\n",
    "        df['segment_id'] = df['segment_id'].replace({-1:segment_id_counter})\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def upsample_and_interpolate(df: pd.DataFrame, time_column:str='time', method:str='linear', limit_direction:str='forward'):\n",
    "        # Since the delta_time column is no longer needed to detect discontinuities,\n",
    "        # Drop delta_time so we can rebuild it at a segment_id level\n",
    "        df.drop(['delta_time'], axis=1, inplace=True)\n",
    "        \n",
    "        # set the timestamp as the index for the dataframe\n",
    "        kwargs = dict(method=method, limit_direction=limit_direction)\n",
    "        if method=='spline':\n",
    "            kwargs['order']=2\n",
    "        df = df.set_index(time_column).copy()\n",
    "        df = df.resample('s').interpolate(**kwargs).reset_index()\n",
    "        return df\n",
    "    \n",
    "    def normalize_sampling_rate(self, df: pd.DataFrame, partition_column:str='segment_id') -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        functions_to_apply = [self.upsample_and_interpolate, self.enrich_time_data]\n",
    "        for func in functions_to_apply:\n",
    "            df = pd.concat(list(map(func, [df_group for _,df_group in df.groupby(partition_column)])), ignore_index=True).sort_index()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampler = TimeUpsampler()\n",
    "df_jan = upsampler.process(df_jan, upsample=True)\n",
    "df_apr = upsampler.process(df_apr, upsample=True)\n",
    "df_may = upsampler.process(df_may, upsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "class BasicEnricher:\n",
    "    def process(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        process_steps = [self.compute_distance, self.compute_heading, self.compute_speed, self.flag_cruising_rows, \n",
    "                         self.convert_elevation, self.compute_grade, self.compute_cumulative_elevation_changes]\n",
    "        \n",
    "        df_enriched = reduce(lambda df,func:func(df), process_steps, df)\n",
    "        return df_enriched\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Distance and Heading from Lat/Long\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def compute_distance(df, latitude='latitude', longitude='longitude', fill_first=0.0):\n",
    "        df = df.copy()\n",
    "        # Copy the previous values of Lat/Long to the current row for vectorized computation\n",
    "        df['lat_old'] = df[latitude].shift()\n",
    "        df['long_old'] = df[longitude].shift()\n",
    "        \n",
    "        # Grab the relevant columns for distance calculation\n",
    "        df_gps = df[['lat_old', 'long_old', latitude, longitude]]\n",
    "        \n",
    "        # Define an anonymous function to execute over each row to calculate the distance between rows\n",
    "        # Units should be in feet to prevent floating point precision issues for other calculations. (lower noise)\n",
    "        haversine_distance = lambda x: max(0,haversine((x.iloc[0], x.iloc[1]), (x.iloc[2], x.iloc[3]), unit='ft'))\n",
    "        \n",
    "        # Create the distance column, making sure to apply the function row-by-row\n",
    "        df['delta_dist_ft'] = df_gps.apply(haversine_distance, axis=1)\n",
    "        df['delta_dist_ft'] = df['delta_dist_ft'].fillna(fill_first)\n",
    "        \n",
    "        # Remove the old latitude and longitude columns\n",
    "        df.drop(['lat_old','long_old'], axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_heading(df, latitude='latitude', longitude='longitude'):\n",
    "        df = df.copy()\n",
    "        # Copy the previous values of Lat/Long to the current row for vectorized computation\n",
    "        df['lat_old'] = df[latitude].shift()\n",
    "        df['long_old'] = df[longitude].shift()\n",
    "        \n",
    "        # Grab the relevant columns for distance calculation\n",
    "        df_gps = df[['lat_old', 'long_old', latitude, longitude]]\n",
    "        \n",
    "        # Define an anonymous function to execute over each row to calculate the angle with North as 0 degrees\n",
    "        # NOTE: we use \"delta_lat / delta_long\" to ensure that North = 0 degrees\n",
    "        rad2deg = 180.0 / np.pi\n",
    "        heading = lambda x: rad2deg * np.arctan2((x.iloc[2]-x.iloc[0]), (x.iloc[3]-x.iloc[1])) # atan(delta_lat / delta_long)\n",
    "        \n",
    "        # Create the distance column, making sure to apply the function row-by-row\n",
    "        df['heading'] = df_gps.apply(heading, axis=1)\n",
    "        df['heading'] = df['heading'].apply(lambda x: x + 360.0*(1-np.sign(x))/2) # correct for negative angles\n",
    "        \n",
    "        # Remove the old latitude and longitude columns\n",
    "        df.drop(['lat_old','long_old'], axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Speed Enrichments\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def compute_speed(df):\n",
    "        df = df.copy()\n",
    "        feet_to_miles = 1.0 / 5280.0\n",
    "        miles_per_second_2_MPH = 3600.0 / 1.0 # conversion factor\n",
    "        df['speed'] = miles_per_second_2_MPH * (feet_to_miles*df['delta_dist_ft']) / df['delta_time']\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def flag_cruising_rows(df, start_threshold_mph:float=8.0, stop_threshold_mph:float=5.0):\n",
    "        \"\"\"\n",
    "        Scmitt Trigger to implement a hysteresis state machine for determining a state\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['is_cruising'] = False\n",
    "\n",
    "        for k in range(1, df.shape[0]):\n",
    "            previous_state = df.loc[k-1,'is_cruising']\n",
    "            current_speed = df.loc[k,'speed']\n",
    "            if (previous_state==False) & (current_speed >= start_threshold_mph):\n",
    "                df.loc[k,'is_cruising'] = True # rising threshold surpassed\n",
    "            elif (previous_state==True) & (current_speed < stop_threshold_mph):\n",
    "                df.loc[k,'is_cruising'] = False # rising threshold surpassed\n",
    "            else:\n",
    "                # if there is no change, propogate the previous state\n",
    "                df.loc[k,'is_cruising'] = df.loc[k-1,'is_cruising']\n",
    "        return df\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Elevation Enrichments\n",
    "    ##############################################################################################\n",
    "    @staticmethod\n",
    "    def convert_elevation(df):\n",
    "        df = df.copy()\n",
    "        meters_to_feet = 3.281\n",
    "        df['elevation'] = df['elevation'] * meters_to_feet\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_grade(df):\n",
    "        df = df.copy()\n",
    "        fill_first = 0.0\n",
    "        df['delta_ele_ft'] = df['elevation'].diff()\n",
    "        df['delta_ele_ft'] = df['delta_ele_ft'].fillna(fill_first)\n",
    "        df['grade'] = 100.0 * (df['delta_ele_ft'] / df['delta_dist_ft'])\n",
    "        df.loc[0,'grade'] = 0.0 # initialize and assumed 0% slope as the starting point--representative of a typical parking lot\n",
    "        df.loc[~np.isfinite(df['grade']),'grade'] = np.nan\n",
    "\n",
    "        # fill in nulls where delta_dist==0.0 by interpolating the value. If you stop on a hill, your grade should carry forward\n",
    "        df['grade'] = df['grade'].interpolate('linear')\n",
    "\n",
    "        # Constrain Grade to be within the typical +/- 15 % all riders deal with. We'll use 18% as thresholds\n",
    "        df['grade_saturated'] = df['grade'].apply(lambda g: min(g,18)).apply(lambda g: max(g,-18))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cumulative_elevation_changes(df, fill_first=0.0):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # create an elevation difference\n",
    "        df['delta_ele'] = df['elevation'].diff()\n",
    "        df['delta_ele'] = df['delta_ele'].fillna(fill_first)\n",
    "        \n",
    "        # create delta ascent and delta descent columns\n",
    "        df['delta_ascent'] = df.loc[df['delta_ele']>=0, 'delta_ele']\n",
    "        df['delta_descent'] = df.loc[df['delta_ele']<=0, 'delta_ele']\n",
    "        \n",
    "        # create the cumulative versions\n",
    "        df['elapsed_ascent'] = df['delta_ascent'].cumsum()\n",
    "        df['elapsed_ascent'] = df['elapsed_ascent'].interpolate() # fill in any blanks\n",
    "        df['elapsed_descent'] = df['delta_descent'].cumsum()\n",
    "        df['elapsed_descent'] = np.abs(df['elapsed_descent'].interpolate()) # fill in any blanks\n",
    "        \n",
    "        # create the total elevation change column\n",
    "        df['elapsed_elevation'] = df['elapsed_ascent'] + df['elapsed_descent']\n",
    "            \n",
    "        # drop the elevation differences\n",
    "        df.drop(['delta_ele','delta_ascent','delta_descent'], axis=1, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "class PowerEstimator:\n",
    "    def __init__(self, power_params:dict, speed_col:str='filt_speed', grade_col:str='filt_grade_saturated')->None:\n",
    "        self.power_params = power_params\n",
    "        self.speed_col = speed_col\n",
    "        self.grade_col = grade_col\n",
    "\n",
    "    def estimate_power(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        return self.get_instantaneous_power(df, self.power_params, self.speed_col, self.grade_col)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instantaneous_power(df:pd.DataFrame, power_params:dict, speed_column:str='speed', grade_column:str='grade_saturated'):\n",
    "        cols_to_drop_later = ['grade_radians','speed_MpS','F_grav','F_fric','F_drag', 'F_sum', \n",
    "                            'total_speed']\n",
    "        params = power_params\n",
    "\n",
    "        # Convert the terrain slope into radians\n",
    "        df['grade_radians'] = np.arctan(df[grade_column]/100)\n",
    "        \n",
    "        # Convert the speed units into meters per second\n",
    "        mph2MpS = 0.44704 # 1 MPH = 0.44704 m/s\n",
    "        df['speed_MpS'] = mph2MpS * df[speed_column]\n",
    "        \n",
    "        # Get the total speed component with wind (placeholder)\n",
    "        df['total_speed'] = df['speed_MpS']\n",
    "        \n",
    "        # Calculate the individual forces\n",
    "        pounds_to_kilograms = 0.453592\n",
    "        total_mass = pounds_to_kilograms * sum(list(params['weight'].values()))\n",
    "        df['F_grav'] = total_mass*params['gravity'] * np.sin(df['grade_radians'])\n",
    "        df['F_fric'] = params['mu_rr']*total_mass*params['gravity'] * np.cos(df['grade_radians'])\n",
    "        full_coefficient = 0.5 * params['rho_air'] * params['area'] * params['c_drag']\n",
    "        df['F_drag'] = (full_coefficient) * np.power(df['total_speed'], 2) # k(v)^2\n",
    "        \n",
    "        # Sum the forces\n",
    "        df['F_sum'] = df['F_drag'] + df['F_grav'] + df['F_fric']\n",
    "        \n",
    "        # Calculate the non-negative power delivered by the ride (set Power=0 for F_sum <0)\n",
    "        df['inst_power'] = (1.0/params['eta_dt']) * df['F_sum'] * df['speed_MpS'] \n",
    "        df.loc[df['inst_power']<0,'inst_power'] = 0 # coasting when sum of forces is negative (no input power)\n",
    "\n",
    "        df.drop(columns=cols_to_drop_later, inplace=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "enricher = BasicEnricher()\n",
    "df_jan = enricher.process(df_jan)\n",
    "df_apr = enricher.process(df_apr)\n",
    "df_may = enricher.process(df_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b400f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "class NoiseFilter:\n",
    "    def __init__(self, filter_type:str='hann', filter_order:int=21):\n",
    "        if filter_type=='hann':\n",
    "            fir_filter = signal.windows.hann(filter_order)\n",
    "        elif filter_type=='blackman-harris':\n",
    "            fir_filter = signal.windows.blackmanharris(filter_order)\n",
    "        elif filter_type=='rect':\n",
    "            fir_filter = signal.windows.boxcar(filter_order)\n",
    "        self.fir_filter = fir_filter\n",
    "\n",
    "    def filter_columns(self, df:pd.DataFrame, columns:list[str]) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for col in columns:\n",
    "            df = self.apply_filter(df, col, self.fir_filter)\n",
    "        return df \n",
    "\n",
    "    @staticmethod\n",
    "    def apply_filter(df:pd.DataFrame, signal_column:str, fir_filter:np.ndarray) -> pd.DataFrame:\n",
    "        signal_list = list(df[signal_column].values)\n",
    "        filtered_signal = signal.convolve(signal_list, fir_filter, mode='same') / sum(fir_filter)\n",
    "        \n",
    "        df['filt_'+signal_column] = np.nan\n",
    "        if len(signal_list) == len(filtered_signal):\n",
    "            df['filt_'+signal_column] = filtered_signal\n",
    "        else:\n",
    "            df.loc[1:,'filt_'+signal_column] = filtered_signal\n",
    "            df.loc[0,'filt_'+signal_column] = df.loc[1,'filt_'+signal_column] # backfill\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_filter = NoiseFilter()\n",
    "columns_to_filter = ['hr', 'speed', 'grade', 'grade_saturated']\n",
    "df_jan = noise_filter.filter_columns(df_jan, columns_to_filter)\n",
    "df_apr = noise_filter.filter_columns(df_apr, columns_to_filter)\n",
    "df_may = noise_filter.filter_columns(df_may, columns_to_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94491d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_params = {'weight': {'rider':220, # needs to be converted lb-->kg\n",
    "                   'bike':25,\n",
    "                   'bags':5\n",
    "                  }, \n",
    "                  'area': 0.4635862, # m^2\n",
    "                  'mu_rr': 0.005, # coefficient of rolling friction\n",
    "                  'c_drag': 0.95, # coefficient of drag\n",
    "                  'rho_air': 1.2, # kg/m^3 air density\n",
    "                  'eta_dt': 0.96, # efficiency of drive train\n",
    "                  'gravity': 9.8 # m/s^2\n",
    "                 }\n",
    "\n",
    "pwr_estimator = PowerEstimator(power_params, speed_col='filt_speed', grade_col='filt_grade_saturated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef755314",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'ride_id':555, 'b_weight':50, 'a_weight':100, 'c_weight':10, 'other':20000},\n",
    "{'ride_id':222, 'b_weight':4, 'a_weight':5, 'c_weight':6, 'other':777}]\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa989f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan = pwr_estimator.estimate_power(df_jan)\n",
    "df_apr = pwr_estimator.estimate_power(df_apr)\n",
    "df_may = pwr_estimator.estimate_power(df_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2564c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_column(df_apr,'inst_power', units='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708875a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRETS_CONFIG = Path().cwd().parent.parent / \"etl/sensitive_config/secrets.yaml\"\n",
    "import yaml\n",
    "\n",
    "with open(SECRETS_CONFIG, 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26439adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivacyZoner():\n",
    "    def __init__(self, privacy_zone_config:dict):\n",
    "        self.df = None\n",
    "        self.privacy_zone_config = privacy_zone_config\n",
    "        self.df_privacy = None\n",
    "        self.temporary_prox_columns = []\n",
    "\n",
    "    def process(self, df:pd.DataFrame) -> pd.DataFrame:\n",
    "        self.df = df.copy()\n",
    "        self._read_privacy_zones()\n",
    "        self._calculate_proximities()\n",
    "        self._remove_violation_gps_data()\n",
    "        self._drop_temporary_prox_columns()\n",
    "        return self.df\n",
    "\n",
    "    ################################################################\n",
    "    # PROCESS METHODS\n",
    "    ################################################################\n",
    "\n",
    "    def _read_privacy_zones(self):\n",
    "        data = []\n",
    "        for private_location in self.privacy_zone_config.keys():\n",
    "            datum = {'name':private_location, 'latitude':self.privacy_zone_config[private_location]['lat'],\n",
    "                     'longitude':self.privacy_zone_config[private_location]['long'], \n",
    "                     'privacy_radius':self.privacy_zone_config[private_location]['scrub_radius']}\n",
    "            data.append(datum)\n",
    "        self.df_privacy = pd.DataFrame(data)\n",
    "\n",
    "    def _calculate_proximities(self):\n",
    "        for private_location in range(self.df_privacy.shape[0]):\n",
    "            # get the relevant parameters\n",
    "            dist_name = 'prox_' + self.df_privacy.loc[private_location, 'name']\n",
    "            latitude = self.df_privacy.loc[private_location, 'latitude']\n",
    "            longitude = self.df_privacy.loc[private_location, 'longitude']\n",
    "            \n",
    "            # calculate the proximity\n",
    "            self.df[dist_name] = self._get_proximity_to_address(latitude, longitude)\n",
    "            self.temporary_prox_columns.append(dist_name)\n",
    "\n",
    "    def _remove_violation_gps_data(self):\n",
    "        for private_location in range(self.df_privacy.shape[0]):\n",
    "             # get the relevant parameters\n",
    "            dist_name = 'prox_' + self.df_privacy.loc[private_location, 'name']\n",
    "            privacy_radius = self.df_privacy.loc[private_location, 'privacy_radius']\n",
    "\n",
    "            filt_violation = self.df.loc[:,dist_name] <= privacy_radius\n",
    "            self.df.loc[filt_violation, 'latitude'] = np.nan\n",
    "            self.df.loc[filt_violation, 'longitude'] = np.nan\n",
    "\n",
    "    def _drop_temporary_prox_columns(self):\n",
    "        self.df.drop(self.temporary_prox_columns, axis=1, inplace=True)\n",
    "\n",
    "    ################################################################\n",
    "    # HELPER METHODS\n",
    "    ################################################################\n",
    "\n",
    "    def _get_proximity_to_address(self, addr_latitude, addr_longitude):\n",
    "        df_gps = self.df[['latitude', 'longitude']]\n",
    "        \n",
    "        # Define an anonymous function to execute over each row to calculate the distance between rows\n",
    "        haversine_distance = lambda x: haversine((x.iloc[0], x.iloc[1]), (addr_latitude, addr_longitude), unit='mi')\n",
    "        \n",
    "        # Create the distance column, making sure to apply the function row-by-row\n",
    "        proximity = df_gps.apply(haversine_distance, axis=1)\n",
    "        \n",
    "        return proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRETS_CONFIG = Path().cwd().parent.parent / \"etl/sensitive_config/secrets.yaml\"\n",
    "import yaml\n",
    "\n",
    "with open(SECRETS_CONFIG, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "zoner = PrivacyZoner(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan = zoner.process(df_jan)\n",
    "df_apr = zoner.process(df_apr)\n",
    "df_may = zoner.process(df_may)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5772a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x67faa109'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(hash(df_apr.time[0].timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c80ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
